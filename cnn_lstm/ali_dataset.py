import os
import torch
import numpy as np
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from GloVe.glove import GloVeEmbeddings

# 数据集文件路径
data_dir = os.path.join('data','customized_ali_data.csv')
# data_dir = os.path.join('data','test.csv')

class ALiDataSet(Dataset):
    def __init__(self, train=True,sequence_max_len = 50):
        self.sequence_max_len = sequence_max_len
        self.glove = GloVeEmbeddings()
        self.df = pd.read_csv(data_dir)

        train_df, test_df = train_test_split(self.df, test_size=0.1, random_state=42)
        if train:
            self.df = train_df
        else:
            self.df = test_df
    
    def __getitem__(self, index):
        
        api_list = tokenlize(self.df.iloc[index]['cleaned_words'])
        label = self.df.iloc[index]['label']

        api_idx_list = self.glove.word2idx(api_list, self.sequence_max_len)
        api_vector_list = self.glove.idx2vector(api_idx_list)

        return api_vector_list, label
    
    def __len__(self):
        return len(self.df)


def tokenlize(sentence):
    """
    进行文本分词
    :param sentence: strs
    :return: [str,str,str]
    """
    # fileters = ['!', '"', '#', '$', '%', '&', '\(', '\)', '\*', '\+', ',', '-', '\.', '/', ':', ';', '<', '=', '>',
    #             '\?', '@', '\[', '\\', '\]', '^', '_', '`', '\{', '\|', '\}', '~', '\t', '\n', '\x97', '\x96', '”',
    #             '“', ]
    # sentence = sentence.lower()  # 把大写转化为小写
    # sentence = re.sub("<br />", " ", sentence)
    # sentence = re.sub("|".join(fileters), " ", sentence)
    result = [i for i in sentence.split(" ") if len(i) > 0]

    return result

def collate_fn(batch):
    api_vector_list,label = zip(*batch)
    
    # return torch.tensor(api_vector_list,dtype=torch.float32), torch.tensor(label,dtype=torch.float32)
    return torch.tensor(np.array(api_vector_list),dtype=torch.float32), torch.tensor(np.array(label))

def ALiData(batch_size, train, sequence_max_len):
    ali_dataset = ALiDataSet(train=train, sequence_max_len=sequence_max_len)
    ali_dataloader = DataLoader(ali_dataset, batch_size=batch_size,collate_fn=collate_fn)
    return ali_dataloader
